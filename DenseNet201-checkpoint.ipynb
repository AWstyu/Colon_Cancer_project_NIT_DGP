{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7ec58-4fdf-4aa7-b5f1-4f8751158647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310ae78-0438-4638-bb25-442b929010fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\COLON_CANCER DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903be99c-188a-4fb4-bae6-d278ad2e5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip  install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170046e-753e-4c55-84d1-6aa296e9f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def augment_images(input_dir, output_dir, num_augmented=5):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        A.Rotate(limit=45, p=0.7),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=30, p=0.5),\n",
    "        A.ElasticTransform(alpha=1, sigma=50, p=0.3),  \n",
    "        A.GridDistortion(p=0.3),\n",
    "        A.CLAHE(clip_limit=2, tile_grid_size=(8,8), p=0.3)\n",
    "    ])\n",
    "    \n",
    "    for img_name in tqdm(os.listdir(input_dir)):\n",
    "        img_path = os.path.join(input_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(num_augmented):\n",
    "            augmented = transform(image=image)['image']\n",
    "            augmented = np.clip(augmented, 0, 255).astype(np.uint8)  \n",
    "            \n",
    "            save_path = os.path.join(output_dir, f\"aug_{i}_{img_name}\")\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "        \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\COMBINED_PICS\"  \n",
    "    output_folder = \"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\COMBINED_PICS_AUG\"  \n",
    "    augment_images(input_folder, output_folder, num_augmented=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd658f67-30a4-46f7-9cdd-949fac5b045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_dataset(input_dir, output_base_dir, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    assert train_ratio + val_ratio + test_ratio == 1\n",
    "    \n",
    "    train_dir = os.path.join(output_base_dir, \"train\")\n",
    "    val_dir = os.path.join(output_base_dir, \"val\")\n",
    "    test_dir = os.path.join(output_base_dir, \"test\")\n",
    "    \n",
    "    for folder in [train_dir, val_dir, test_dir]:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "    \n",
    "    image_files = os.listdir(input_dir)\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    train_split = int(len(image_files) * train_ratio)\n",
    "    val_split = int(len(image_files) * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_files = image_files[:train_split]\n",
    "    val_files = image_files[train_split:val_split]\n",
    "    test_files = image_files[val_split:]\n",
    "    \n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(input_dir, file), os.path.join(train_dir, file))\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(input_dir, file), os.path.join(val_dir, file))\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(input_dir, file), os.path.join(test_dir, file))\n",
    "    \n",
    "    print(f\"Dataset split complete: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\COMBINED_PICS_AUG\" \n",
    "    dataset_split_folder = \"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\"  \n",
    "    \n",
    "    split_dataset(input_folder, dataset_split_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476116bd-2339-40ca-a6cd-46dcc3d78f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_excel(image_dir, output_excel):\n",
    "    label_map = {\"BLI\": 0, \"FICE\": 1, \"LCI\": 2, \"NBI\": 3, \"WLI\": 4}\n",
    "    data = []\n",
    "    \n",
    "    for img_name in os.listdir(image_dir):\n",
    "        for key in label_map.keys():\n",
    "            if key in img_name:\n",
    "                data.append([img_name, label_map[key]])\n",
    "                break\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\"Image_Name\", \"Label\"])\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Excel file saved at {output_excel}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_folder = \"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\COMBINED_PICS_AUG\" \n",
    "    excel_output_path = \"image_labels.xlsx\"\n",
    "    create_excel(image_folder, excel_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f207b-4399-4d11-b3d6-662b8d926e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "model = DenseNet201(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "\n",
    "image_folder = 'C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\train'\n",
    "\n",
    "\n",
    "output_folder = os.path.join(os.path.dirname(image_folder), 'Feature_PKLs')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_features(image_path, model):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "    features = model.predict(img_data)\n",
    "    return features\n",
    "\n",
    "\n",
    "for img_file in tqdm(os.listdir(image_folder)):\n",
    "    img_path = os.path.join(image_folder, img_file)\n",
    "    if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        features = extract_features(img_path, model)\n",
    "        feature_vector = features.flatten()\n",
    "        \n",
    "        \n",
    "        feature_filename = os.path.join(output_folder, f\"{os.path.splitext(img_file)[0]}_features_densenet.pkl\")\n",
    "        with open(feature_filename, 'wb') as f:\n",
    "            pickle.dump(feature_vector, f)\n",
    "        print(f\"Features saved for {img_file}!\")\n",
    "\n",
    "print(\"Feature extraction complete. Features saved to individual .pkl files in the folder:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc044bf4-580c-421b-a8aa-84f88752a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "model = DenseNet201(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "\n",
    "image_folder = 'C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\test'\n",
    "\n",
    "\n",
    "output_folder = os.path.join(os.path.dirname(image_folder), 'Test_Feature_PKLs')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_features(image_path, model):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "    features = model.predict(img_data)\n",
    "    return features\n",
    "\n",
    "\n",
    "for img_file in tqdm(os.listdir(image_folder)):\n",
    "    img_path = os.path.join(image_folder, img_file)\n",
    "    if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        features = extract_features(img_path, model)\n",
    "        feature_vector = features.flatten()\n",
    "        \n",
    "       \n",
    "        feature_filename = os.path.join(output_folder, f\"{os.path.splitext(img_file)[0]}_features_densenet.pkl\")\n",
    "        with open(feature_filename, 'wb') as f:\n",
    "            pickle.dump(feature_vector, f)\n",
    "        print(f\"Features saved for {img_file}!\")\n",
    "\n",
    "print(\"Feature extraction complete. Features saved to individual .pkl files in the folder:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839a749-7a83-40a6-a3fe-1aab52816679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "model = DenseNet201(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "\n",
    "image_folder = 'C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\val'\n",
    "\n",
    "\n",
    "output_folder = os.path.join(os.path.dirname(image_folder), 'Validation_Feature_PKLs')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_features(image_path, model):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "    features = model.predict(img_data)\n",
    "    return features\n",
    "\n",
    "\n",
    "for img_file in tqdm(os.listdir(image_folder)):\n",
    "    img_path = os.path.join(image_folder, img_file)\n",
    "    if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        features = extract_features(img_path, model)\n",
    "        feature_vector = features.flatten()\n",
    "        \n",
    "       \n",
    "        feature_filename = os.path.join(output_folder, f\"{os.path.splitext(img_file)[0]}_features_densenet.pkl\")\n",
    "        with open(feature_filename, 'wb') as f:\n",
    "            pickle.dump(feature_vector, f)\n",
    "        print(f\"Features saved for {img_file}!\")\n",
    "\n",
    "print(\"Feature extraction complete. Features saved to individual .pkl files in the folder:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123973f-06b1-4068-bdf9-ac0d56b29017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename=\"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\Train_Feature_PKLs\\\\aug_1_WLI_01_features_densenet.pkl\"\n",
    "\n",
    "with open(filename,'rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a777bd-f57a-4c4b-afbb-fbeef55cb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "directory=\"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\Train_Feature_PKLs\"\n",
    "\n",
    "con_features=[]\n",
    "count=0\n",
    "\n",
    "for files in os.listdir(directory):\n",
    "    filename=os.path.join(directory,files)\n",
    "    count+=1\n",
    "    with open(filename,'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        l=len(data)\n",
    "        con_features.append(data)\n",
    "\n",
    "con_features_arr=np.concatenate(con_features,axis=0)\n",
    "print(con_features_arr.shape)\n",
    "con_features_mod=con_features_arr.reshape(count,l)\n",
    "\n",
    "print(con_features_mod.shape)\n",
    "\n",
    "np.save(\"CONCATENATED_FEATURES_TRAIN.npy\",con_features_mod)\n",
    "\n",
    "print(\"Filename saved succesfully!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f461577-d2b8-461b-95bb-b4194d747a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "directory=\"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\Validation_Feature_PKLs\"\n",
    "\n",
    "con_features=[]\n",
    "count=0\n",
    "\n",
    "for files in os.listdir(directory):\n",
    "    filename=os.path.join(directory,files)\n",
    "    count+=1\n",
    "    with open(filename,'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        l=len(data)\n",
    "        con_features.append(data)\n",
    "\n",
    "con_features_arr=np.concatenate(con_features,axis=0)\n",
    "print(con_features_arr.shape)\n",
    "con_features_mod=con_features_arr.reshape(count,l)\n",
    "\n",
    "print(con_features_mod.shape)\n",
    "\n",
    "np.save(\"CONCATENATED_FEATURES_VALIDATE.npy\",con_features_mod)\n",
    "\n",
    "print(\"Filename saved succesfully!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ecda2-1afb-4d1b-aab9-9b45703125a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "directory=\"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\Test_Feature_PKLs\"\n",
    "\n",
    "con_features=[]\n",
    "count=0\n",
    "\n",
    "for files in os.listdir(directory):\n",
    "    filename=os.path.join(directory,files)\n",
    "    count+=1\n",
    "    with open(filename,'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        l=len(data)\n",
    "        con_features.append(data)\n",
    "\n",
    "con_features_arr=np.concatenate(con_features,axis=0)\n",
    "print(con_features_arr.shape)\n",
    "con_features_mod=con_features_arr.reshape(count,l)\n",
    "\n",
    "print(con_features_mod.shape)\n",
    "\n",
    "np.save(\"CONCATENATED_FEATURES_TEST.npy\",con_features_mod)\n",
    "\n",
    "print(\"Filename saved succesfully!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1453e0-e371-4093-a956-6bf048b10a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"image_labels.xlsx\")\n",
    "\n",
    "\n",
    "directory = \"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\train\"\n",
    "train_files = set(os.listdir(directory)) \n",
    "\n",
    "\n",
    "label_train = df[df[\"Image_Name\"].isin(train_files)][\"Label\"].tolist()\n",
    "\n",
    "print(f\"Total matched labels: {len(label_train)}\")\n",
    "\n",
    "label_train_arr=np.array(label_train)\n",
    "\n",
    "np.save(\"Training_labels.npy\",label_train_arr)\n",
    "\n",
    "print(\"Training labels saved successfully!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1c8d6-5034-4348-974d-211085fa6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"image_labels.xlsx\")\n",
    "\n",
    "\n",
    "directory = \"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\val\"\n",
    "validate_files = set(os.listdir(directory)) \n",
    "\n",
    "\n",
    "label_validate = df[df[\"Image_Name\"].isin(train_files)][\"Label\"].tolist()\n",
    "\n",
    "print(f\"Total matched labels: {len(label_validate)}\")\n",
    "\n",
    "label_validate_arr=np.array(label_validate)\n",
    "\n",
    "np.save(\"Validation_labels.npy\",label_validate_arr)\n",
    "\n",
    "print(\"Validation labels saved successfully!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42d38e-a8d3-4724-b2bb-f04891b7b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "directory = \"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\Test_Feature_Pkls\"\n",
    "\n",
    "\n",
    "modalities = [\"bli\", \"wli\", \"nbi\", \"fice\", \"lci\"]\n",
    "\n",
    "\n",
    "for modality in modalities:\n",
    "    con_features = []\n",
    "    count = 0\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if modality in file.lower():  \n",
    "            filename = os.path.join(directory, file)\n",
    "            count += 1\n",
    "            with open(filename, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                l = len(data)\n",
    "                con_features.append(data)\n",
    "\n",
    "    if count > 0:\n",
    "       \n",
    "        con_features_arr = np.concatenate(con_features, axis=0)\n",
    "        con_features_mod = con_features_arr.reshape(count, l)\n",
    "\n",
    "        \n",
    "        np.save(f\"CONCATENATED_FEATURES_TEST_{modality.upper()}.npy\", con_features_mod)\n",
    "\n",
    "        print(f\"Saved {modality.upper()} test features successfully! Shape: {con_features_mod.shape}\")\n",
    "    else:\n",
    "        print(f\"No files found for {modality.upper()}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4952355-4b9b-4650-b5b1-a4545418f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"image_labels.xlsx\")\n",
    "\n",
    "\n",
    "directory = \"C:\\\\COLON_CANCER DATASET\\\\PolypDB\\\\PolypDB_modality_wise\\\\AUG_PICS\\\\test\"\n",
    "\n",
    "\n",
    "modalities = [\"bli\", \"wli\", \"nbi\", \"fice\", \"lci\"]\n",
    "\n",
    "test_files = set(os.listdir(directory))\n",
    "\n",
    "\n",
    "for modality in modalities:\n",
    "    \n",
    "    modality_test_files = {file for file in test_files if modality in file.lower()}\n",
    "\n",
    "    \n",
    "    label_test = df[df[\"Image_Name\"].isin(modality_test_files)][\"Label\"].tolist()\n",
    "\n",
    "   \n",
    "    label_test_arr = np.array(label_test)\n",
    "    np.save(f\"{modality.upper()}_TEST_LABELS.npy\", label_test_arr)\n",
    "\n",
    "   \n",
    "    print(f\"Total matched {modality.upper()} test labels: {len(label_test)}\")\n",
    "    print(f\"{modality.upper()} test labels saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533badb-3535-49de-838e-1c4a60f00ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X_train = np.load(\"CONCATENATED_FEATURES_TRAIN.npy\")  \n",
    "y_train = np.load(\"Training_labels.npy\")    \n",
    "X_val = np.load(\"CONCATENATED_FEATURES_VALIDATE.npy\")\n",
    "y_val = np.load(\"Validation_labels.npy\")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "\n",
    "num_classes = 5  \n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def build_mlp(input_shape=(1920,), num_classes=5):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(1024, kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = layers.BatchNormalization(momentum=0.5)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.6)(x)\n",
    "    \n",
    "    x = layers.Dense(512, kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization(momentum=0.5)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.6)(x)\n",
    "    \n",
    "    x = layers.Dense(256, kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization(momentum=0.5)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.55)(x)\n",
    "    \n",
    "    x = layers.Dense(128, kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization(momentum=0.5)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(64, kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization(momentum=0.5)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(32, kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization(momentum=0.5)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.45)(x)\n",
    "\n",
    "    x = layers.Dense(16, kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization(momentum=0.5)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.35)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)  \n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    optimizer = optimizers.AdamW(learning_rate=0.01, weight_decay=1e-4, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_mlp()\n",
    "    model.summary()\n",
    "    \n",
    "    lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32, callbacks=[lr_scheduler])\n",
    "    \n",
    "    \n",
    "    log_data = []\n",
    "    for epoch in range(0, len(history.history['accuracy']), 10):\n",
    "        log_data.append([\n",
    "            epoch,\n",
    "            history.history['accuracy'][epoch],\n",
    "            history.history['loss'][epoch],\n",
    "            history.history['val_accuracy'][epoch],\n",
    "            history.history['val_loss'][epoch]\n",
    "        ])\n",
    "\n",
    "    if (len(history.history['accuracy']) - 1) % 10 != 0:\n",
    "        last_epoch = len(history.history['accuracy']) - 1\n",
    "        log_data.append([\n",
    "            last_epoch,\n",
    "            history.history['accuracy'][last_epoch],\n",
    "            history.history['loss'][last_epoch],\n",
    "            history.history['val_accuracy'][last_epoch],\n",
    "            history.history['val_loss'][last_epoch]\n",
    "        ])\n",
    "    \n",
    "   \n",
    "    log_df = pd.DataFrame(log_data, columns=['Epoch', 'Training Accuracy', 'Training Loss', 'Validation Accuracy', 'Validation Loss'])\n",
    "    print(log_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c38be-1f0e-41c6-a9b6-7a8a336c017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.save(\"mlp_model_Densenet201.h5\")\n",
    "\n",
    "print(\"Model saved successfully!!\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy for DenseNet201')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb55b86-ca7e-4b88-9d02-b1aabb8b15e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, log_loss\n",
    "\n",
    "\n",
    "model = load_model(\"mlp_model_Densenet201.h5\")\n",
    "\n",
    "\n",
    "modalities = [\"BLI\", \"WLI\", \"FICE\", \"LCI\", \"NBI\"]\n",
    "label_order = [\"BLI\", \"FICE\", \"LCI\", \"NBI\", \"WLI\"]\n",
    "label_map = {modality: idx for idx, modality in enumerate(label_order)}\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, modality):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(5))\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=label_order, yticklabels=label_order)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix - {modality}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "l_dense = []\n",
    "\n",
    "\n",
    "for modality in modalities:\n",
    "    X_test = np.load(f\"CONCATENATED_FEATURES_TEST_{modality}.npy\")\n",
    "    \n",
    "   \n",
    "    modality_label = label_map[modality]\n",
    "    y_test = np.full(shape=(len(X_test),), fill_value=modality_label)\n",
    "\n",
    "    num_classes = 5  \n",
    "    y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "   \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    logloss = log_loss(y_test_one_hot, y_pred_probs)\n",
    "    l_dense.append(logloss)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "   \n",
    "    print(f\"Results for {modality}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(f\"Log Loss: {logloss:.4f}\")\n",
    "    print(f\"Unique labels in y_test: {np.unique(y_test)}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    \n",
    "    plot_confusion_matrix(y_true, y_pred, modality)\n",
    "\n",
    "\n",
    "print(f\"Average Log_loss for the DENSENET201: {sum(l_dense)/5:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
